{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-29T16:03:16.084592Z","iopub.execute_input":"2021-08-29T16:03:16.084929Z","iopub.status.idle":"2021-08-29T16:03:16.089006Z","shell.execute_reply.started":"2021-08-29T16:03:16.084898Z","shell.execute_reply":"2021-08-29T16:03:16.087926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel,TFBertForSequenceClassification\nimport numpy as np\nfrom transformers import TFAutoModel\nfrom tensorflow.keras import layers\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, AutoModel\n\n# Load the BERT tokenizer and the pretrained Arabic base BERT model\nbert_tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n# Load the BERT model and the pretrained Arabic base BERT model\nmodel = TFAutoModel.from_pretrained(\"asafaya/bert-base-arabic\")\n#get the liste of sentences\n\"\"\"\nWhen feeding text data into our model, there are a few things to \nbe aware of. First, we must use tokenizer.encode_plus(...) \nto convert our text into input IDs and attention mask tensors\n\"\"\"\nimport tensorflow as tf\n\ndef tokenize_bert(data):\n    sentences = data['setences'].values\n    seq_len = 512\n    num_samples = len(sentences)\n    #lists of ids\n    input_ids = np.zeros((num_samples, seq_len))\n    #get the lists of masks\n    attention_masks = np.zeros((num_samples, seq_len))\n    for i,sent in enumerate(sentences):\n        #tokenize words in the sentence\n        #return_tensors: return the resultat as tensor\n        #pad_to_max_length :pads the sentences to max length\n        bert_inp=bert_tokenizer.encode_plus(sent,add_special_tokens = True\n                                            ,max_length =seq_len,pad_to_max_length = True,\n                                             truncation=True\n                                            ,return_attention_mask = True,\n                                             return_tensors='tf')  # Return pytorch tensors.\n        #add encoded \n        input_ids[i, :] = bert_inp['input_ids']\n        #applying attention masks\n        attention_masks[i, :] = bert_inp['attention_mask']\n    return input_ids,attention_masks\n\n\ndef get_model():\n    # Load the BERT tokenizer and the pretrained Arabic base BERT model\n    bert_tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n    # Load the BERT model and the pretrained Arabic base BERT model\n    model1 = TFAutoModel.from_pretrained(\"asafaya/bert-base-arabic\")\n    model1.trainable = False\n    max_len = 512\n    #'input_ids', 'attention_mask'\n    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n    embeddings = model1.bert(\n            input_ids, attention_mask=attention_mask\n        )[1]\n    x = tf.keras.layers.Dense(519, activation='relu')(embeddings)\n    #output layer\n    y = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(x)\n    model1 = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=y)\n    \n    return model1","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:03:37.066884Z","iopub.execute_input":"2021-08-29T16:03:37.067231Z","iopub.status.idle":"2021-08-29T16:03:40.937401Z","shell.execute_reply.started":"2021-08-29T16:03:37.067193Z","shell.execute_reply":"2021-08-29T16:03:40.936596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neg = \"../input/twitter/Negative\"\npos = \"../input/twitter/Positive\"\npositiveList=[]\nnegativeList=[]\n\nnegative = os.listdir(neg) \npostive = os.listdir(pos)\n\nfor file in negative:\n    with open('../input/twitter/Negative/'+file,encoding=\"UTF-8\") as f:\n        try:\n            negativeList.append(f.read())\n        except:\n            pass\nfor file in postive:\n    with open(pos+\"/\"+file,encoding=\"UTF-8\") as f:\n        try:\n            positiveList.append(f.read())\n        except:\n            pass\nprint(len(positiveList))\nprint(len(negativeList))\ndata={\"setences\":positiveList,\"positive\":[1 for i in range(len(positiveList))]}\ndf = pd.DataFrame.from_dict(data)\ndata1={\"setences\":negativeList,\"positive\":[0 for i in range(len(negativeList))]}\ndf1 =pd.DataFrame.from_dict(data1)\ndf = df.append(df1)\nfrom sklearn.utils import shuffle\nRANDOM_SEED = 14\n#shuffle the data randomly\ndata = shuffle(df, random_state=RANDOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:03:51.824386Z","iopub.execute_input":"2021-08-29T16:03:51.824708Z","iopub.status.idle":"2021-08-29T16:03:54.214795Z","shell.execute_reply.started":"2021-08-29T16:03:51.824675Z","shell.execute_reply":"2021-08-29T16:03:54.213992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nsplit = int((80*len(data))/100)\n#train \ntrain = data[:split]\ntest = data[split:]\n#get the inputs tokens and masks\ntrain_inp,train_mask = tokenize_bert(train)\ntest_inp,test_mask = tokenize_bert(test)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:03:59.283755Z","iopub.execute_input":"2021-08-29T16:03:59.284115Z","iopub.status.idle":"2021-08-29T16:03:59.919224Z","shell.execute_reply.started":"2021-08-29T16:03:59.284081Z","shell.execute_reply":"2021-08-29T16:03:59.918335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(lr=1e-5, decay=1e-6)\n#categorical_crossentropy\nfrom transformers import TFAutoModel\nfrom tensorflow.keras import layers\nimport tensorflow as tf\n# Load the BERT model and the pretrained Arabic base BERT model\nmodel = get_model()\nmodel.summary()\nmodel.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\nmodel.fit([train_inp,train_mask],train['positive'],epochs=16,verbose=2)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:04:02.660655Z","iopub.execute_input":"2021-08-29T16:04:02.660999Z","iopub.status.idle":"2021-08-29T16:14:23.333786Z","shell.execute_reply.started":"2021-08-29T16:04:02.660958Z","shell.execute_reply":"2021-08-29T16:14:23.333014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\ntest_lost ,test_acc_model = model.evaluate(\n[test_inp,test_mask],test['positive']\n)\nprint(\"The accuracy of the model model is:\",(test_acc_model*100))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:14:35.826075Z","iopub.execute_input":"2021-08-29T16:14:35.826417Z","iopub.status.idle":"2021-08-29T16:14:49.182076Z","shell.execute_reply.started":"2021-08-29T16:14:35.826383Z","shell.execute_reply":"2021-08-29T16:14:49.181272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"name=\"BERT sentiment analysis\"\nname = name.replace(' ','-')\nmodel.save(f'{name}.h5')","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:14:53.080061Z","iopub.execute_input":"2021-08-29T16:14:53.080395Z","iopub.status.idle":"2021-08-29T16:14:54.265143Z","shell.execute_reply.started":"2021-08-29T16:14:53.080364Z","shell.execute_reply":"2021-08-29T16:14:54.264181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Rumors analysis","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize,word_tokenize\nfrom transformers import TFAutoModel\nfrom transformers import AutoTokenizer\nimport string\nimport tensorflow as tf\n#text feature exctraction\ndef sentences_count(sentences):\n    tokens = sent_tokenize(sentences)\n    return len(tokens)\ndef word_count(sentences):\n    tokens = word_tokenize(sentences)\n    punctuations = string.punctuation\n    return sum(1 for token in tokens if token not in punctuations)\ndef distanctWords(sentences):\n    tokens = set(word_tokenize(sentences))\n    punctuations = string.punctuation\n    return sum(1 for token in tokens if token not in punctuations)\ndef avg_word_count(sentences):\n    try:\n        sentences = sent_tokenize(sentences)\n        result = sum(word_count(sentence) for sentence in sentences)/len(sentences)\n        return result\n    except:\n        return 0\ndef questionMarks(sentences):\n    return sum(1 for token in sentences if (token ==\"؟\") )\ndef exclamationMarks(sentences):\n    return sum(1 for token in sentences if (token ==\"!\") )\ndef negation(sentences):\n    negationList=['لن','لم','لا','ليس','لستم','ليست','ليسوا']\n    sentences = sent_tokenize(sentences)\n    return sum(1 for token in sentences if token in negationList)\ndef calculValues(sentences):\n    sentCont=sentences_count(sentences)\n    wordCont=word_count(sentences)\n    disWordCont=distanctWords(sentences)\n    agvWordCont=avg_word_count(sentences)\n    quesMark = questionMarks(sentences)\n    exclamationMark = exclamationMarks(sentences)\n    negate = negation(sentences)\n    # Load the BERT model and the pretrained Arabic base BERT model\n    bert_tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n    bert_inp=bert_tokenizer.encode_plus(sentences,add_special_tokens = True\n                                            ,max_length =512,pad_to_max_length = True,\n                                             truncation=True\n                                            ,return_attention_mask = True,\n                                             return_tensors='tf') \n    sent = tf.keras.models.load_model(f'./BERT-sentiment-analysis.h5')\n    sentiment = sent.predict([bert_inp['input_ids'],bert_inp['attention_mask']])\n    sentiment=list(sentiment[0])\n    sentiment_calc = sentiment[0] if sentiment[0] > sentiment[1] else sentiment[1]\n    return [sentCont,wordCont,disWordCont,agvWordCont,quesMark,exclamationMark,negate,sentiment.index(sentiment_calc)]\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:15:01.63053Z","iopub.execute_input":"2021-08-29T16:15:01.630861Z","iopub.status.idle":"2021-08-29T16:15:01.643723Z","shell.execute_reply.started":"2021-08-29T16:15:01.63083Z","shell.execute_reply":"2021-08-29T16:15:01.642916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize,word_tokenize\nfrom transformers import TFAutoModel\nfrom transformers import AutoTokenizer\nimport string\nimport tensorflow as tf\n#text feature exctraction\ndef sentences_count(sentences):\n    tokens = sent_tokenize(sentences)\n    return len(tokens)\ndef word_count(sentences):\n    tokens = word_tokenize(sentences)\n    punctuations = string.punctuation\n    return sum(1 for token in tokens if token not in punctuations)\ndef distanctWords(sentences):\n    tokens = set(word_tokenize(sentences))\n    punctuations = string.punctuation\n    return sum(1 for token in tokens if token not in punctuations)\ndef avg_word_count(sentences):\n    try:\n        sentences = sent_tokenize(sentences)\n        result = sum(word_count(sentence) for sentence in sentences)/len(sentences)\n        return result\n    except:\n        return 0\ndef questionMarks(sentences):\n    return sum(1 for token in sentences if (token ==\"؟\") )\ndef exclamationMarks(sentences):\n    return sum(1 for token in sentences if (token ==\"!\") )\ndef negation(sentences):\n    negationList=['لن','لم','لا','ليس','لستم','ليست','ليسوا']\n    sentences = sent_tokenize(sentences)\n    return sum(1 for token in sentences if token in negationList)\ndef calculValues(sentences):\n    sentCont=sentences_count(sentences)\n    wordCont=word_count(sentences)\n    disWordCont=distanctWords(sentences)\n    agvWordCont=avg_word_count(sentences)\n    quesMark = questionMarks(sentences)\n    exclamationMark = exclamationMarks(sentences)\n    negate = negation(sentences)\n    # Load the BERT model and the pretrained Arabic base BERT model\n    bert_tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n    bert_inp=bert_tokenizer.encode_plus(sentences,add_special_tokens = True\n                                            ,max_length =512,pad_to_max_length = True,\n                                             truncation=True\n                                            ,return_attention_mask = True,\n                                             return_tensors='tf') \n    sent = tf.keras.models.load_model(f'./BERT-sentiment-analysis.h5')\n    sentiment = sent.predict([bert_inp['input_ids'],bert_inp['attention_mask']])\n    sentiment=list(sentiment[0])\n    sentiment_calc = sentiment[0] if sentiment[0] > sentiment[1] else sentiment[1]\n    return [sentCont,wordCont,disWordCont,agvWordCont,quesMark,exclamationMark,negate,sentiment.index(sentiment_calc)]\n","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:15:14.486711Z","iopub.execute_input":"2021-08-29T16:15:14.487061Z","iopub.status.idle":"2021-08-29T16:15:14.501435Z","shell.execute_reply.started":"2021-08-29T16:15:14.487025Z","shell.execute_reply":"2021-08-29T16:15:14.500446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport warnings\nwarnings.simplefilter(\"ignore\")\n#import data from csv file\ndata = pd.read_csv('../input/covidremors/rumours_dataset.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:15:42.29855Z","iopub.execute_input":"2021-08-29T16:15:42.29887Z","iopub.status.idle":"2021-08-29T16:15:42.324277Z","shell.execute_reply.started":"2021-08-29T16:15:42.29884Z","shell.execute_reply":"2021-08-29T16:15:42.323219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#drop unnecery rows\ndata = data.drop([\"post url\",\"post page\",\"post date\",\"likes\",\"shares number\",\"comments\"], axis=1)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:15:46.824723Z","iopub.execute_input":"2021-08-29T16:15:46.825075Z","iopub.status.idle":"2021-08-29T16:15:46.836102Z","shell.execute_reply.started":"2021-08-29T16:15:46.82504Z","shell.execute_reply":"2021-08-29T16:15:46.835247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['post text']=data['post text'].fillna('بدون نص')\ndata[data['post text']=='بدون نص']","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:15:50.222867Z","iopub.execute_input":"2021-08-29T16:15:50.223235Z","iopub.status.idle":"2021-08-29T16:15:50.235218Z","shell.execute_reply.started":"2021-08-29T16:15:50.223202Z","shell.execute_reply":"2021-08-29T16:15:50.234218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encode output","metadata":{}},{"cell_type":"code","source":"data['type'] = data['type'].apply(lambda x: 1 if x=='VRAI' else 0)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:15:53.370613Z","iopub.execute_input":"2021-08-29T16:15:53.370979Z","iopub.status.idle":"2021-08-29T16:15:53.38391Z","shell.execute_reply.started":"2021-08-29T16:15:53.370924Z","shell.execute_reply":"2021-08-29T16:15:53.382772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error=0\ninfos = []\ncols = ['sentencesCount', 'wordCount','distanctWords','avg_word_count','questionMarks','exclamationMarks','negation','sentiment']\nfor i, row in data.iterrows():\n    print(i)\n    try:\n        features = calculValues(row['post text'])\n        infos.append(features)\n    except Exception as e:\n        print(e)\ndf = pd.DataFrame(infos,columns =cols)\ndata1 = pd.concat([data, df], axis=1, join=\"inner\")\n","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:15:59.300917Z","iopub.execute_input":"2021-08-29T16:15:59.301304Z","iopub.status.idle":"2021-08-29T16:33:05.687769Z","shell.execute_reply.started":"2021-08-29T16:15:59.301271Z","shell.execute_reply":"2021-08-29T16:33:05.686937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:33:29.044019Z","iopub.execute_input":"2021-08-29T16:33:29.04433Z","iopub.status.idle":"2021-08-29T16:33:29.061593Z","shell.execute_reply.started":"2021-08-29T16:33:29.044302Z","shell.execute_reply":"2021-08-29T16:33:29.06079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the BERT model and the pretrained Arabic base BERT model\nmodel1 = TFAutoModel.from_pretrained(\"asafaya/bert-base-arabic\")\n\nmodel1.trainable = False  #freeze the weight\n\ninput_ids = layers.Input(shape=(512,), dtype=tf.int32, name='input_ids')\nattention_mask = layers.Input(shape=(512,), dtype=tf.int32, name='attention_mask')\nfeatures = layers.Input(shape=(8,), dtype=tf.float32, name='features1')\nembeddings = model1.bert(\n            input_ids, attention_mask=attention_mask\n        )[1]\nx = tf.keras.layers.Concatenate()([embeddings, features])\nx = tf.keras.layers.Dense(599, activation='relu')(x)\n#output layer\ny = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(x)\nmodel1 = tf.keras.Model(inputs=[input_ids, attention_mask,features], outputs=y)\nmodel1.summary()\n# get the input and the output shapes of the BERT model\nfrom keras.models import Model\nBERT_model = Model(model1.input, model1.output)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:33:42.610877Z","iopub.execute_input":"2021-08-29T16:33:42.611253Z","iopub.status.idle":"2021-08-29T16:33:45.691275Z","shell.execute_reply.started":"2021-08-29T16:33:42.611218Z","shell.execute_reply":"2021-08-29T16:33:45.690422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split = int((80*len(data))/100)\nfrom sklearn.utils import shuffle\n\ndata1 = shuffle(data1)\n#train \ntrain = data1.iloc[:split,:]\ntest = data1.iloc[split:,:]\nimages_train = images=create_dataset(train)\nimages_test = images=create_dataset(test)\ntrain = train.drop(['post image'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:34:18.230242Z","iopub.execute_input":"2021-08-29T16:34:18.230575Z","iopub.status.idle":"2021-08-29T16:34:20.463707Z","shell.execute_reply.started":"2021-08-29T16:34:18.230542Z","shell.execute_reply":"2021-08-29T16:34:20.462847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:34:31.54848Z","iopub.execute_input":"2021-08-29T16:34:31.548824Z","iopub.status.idle":"2021-08-29T16:34:31.569123Z","shell.execute_reply.started":"2021-08-29T16:34:31.548792Z","shell.execute_reply":"2021-08-29T16:34:31.56805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:35:07.198681Z","iopub.execute_input":"2021-08-29T16:35:07.199046Z","iopub.status.idle":"2021-08-29T16:35:07.219928Z","shell.execute_reply.started":"2021-08-29T16:35:07.199009Z","shell.execute_reply":"2021-08-29T16:35:07.219056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# images processing","metadata":{}},{"cell_type":"markdown","source":"## calcul the local binary pattern","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom matplotlib import pyplot as plt\ndef get_pixel(img, center, x, y):\n      \n    new_value = 0\n      \n    try:\n        # If local neighbourhood pixel \n        # value is greater than or equal\n        # to center pixel values then \n        # set it to 1\n        if img[x][y] >= center:\n            new_value = 1      \n    except:\n        pass\n      \n    return new_value\n   \n# Function for calculating LBP\ndef lbp_calculated_pixel(img, x, y):\n   \n    center = img[x][y]\n   \n    val_ar = []\n      \n    # top_left\n    val_ar.append(get_pixel(img, center, x-1, y-1))\n      \n    # top\n    val_ar.append(get_pixel(img, center, x-1, y))\n      \n    # top_right\n    val_ar.append(get_pixel(img, center, x-1, y + 1))\n      \n    # right\n    val_ar.append(get_pixel(img, center, x, y + 1))\n      \n    # bottom_right\n    val_ar.append(get_pixel(img, center, x + 1, y + 1))\n      \n    # bottom\n    val_ar.append(get_pixel(img, center, x + 1, y))\n      \n    # bottom_left\n    val_ar.append(get_pixel(img, center, x + 1, y-1))\n      \n    # left\n    val_ar.append(get_pixel(img, center, x, y-1))\n       \n    # Now, we need to convert binary\n    # values to decimal\n    power_val = [1, 2, 4, 8, 16, 32, 64, 128]\n    val = 0\n    for i in range(len(val_ar)):\n        val += val_ar[i] * power_val[i]    \n    return val\n\ndef LBP(img_bgr):\n    height, width, _ = img_bgr.shape\n    # We need to convert RGB image \n    # into gray one because gray \n    # image has one channel only.\n    img_gray = cv2.cvtColor(img_bgr,cv2.COLOR_BGR2GRAY)\n\n    # Create a numpy array as \n    # the same height and width \n    # of RGB image\n    img_lbp = np.zeros((height, width),\n                       np.uint8)\n\n    for i in range(0, height):\n        for j in range(0, width):\n            img_lbp[i, j] = lbp_calculated_pixel(img_gray, i, j)\n    return img_lbp\n\ndef divide_image_by_block(image):\n    blocks=[]\n    img_x=image.shape[0]\n    img_y=image.shape[1]\n    for x in range(img_x):\n        for y in range(img_y):\n            blocks.append(image[x:x+16][y:y+16])\n    return blocks","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## define the DDT","metadata":{}},{"cell_type":"code","source":"from scipy.fftpack import dct\ndef ddt_for_blocks(image):\n    dct_vectors=[]\n    for x in range(16):\n        dct_vectors.append(dct(dct(image[x]).T).T)\n    return np.array(dct_vectors)\ndef ddt_matrx_std(image):\n    image = LBP(image)\n    image = divide_image_by_block(image)\n    image = ddt_for_blocks(image)\n    ddt_std=[]\n    for x in range(16):\n        ddt_std.append(image[x].std())\n    return ddt_std","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport os\n#load images from the disk\ndef create_dataset(data):\n    #importing images from the disk\n    path=\"../input/covidremors/\"\n    img_data_array=[]\n    class_name=[]\n    #new height and width of images\n    IMG_HEIGHT, IMG_WIDTH = 224,224\n    #iterate throw csv file to get the name of images\n    for index,file in data[['post image']].iterrows():\n        #get image path\n        img_path = data.loc[index,['post image']].values[0]\n        image_path= os.path.join(path,img_path)\n        #read image via opencv\n        image= cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n        #resize the image into IMG_HEIGHT*IMG_WIDTH*3\n        image=cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH ),interpolation = cv2.INTER_AREA)\n        #convert image to numpy array\n        image=np.array(image)\n        #applying DDT on images\n        ddt_std = ddt_matrx_std(image)\n        #convert image to float32\n        image = image.astype('float32')\n        #normalise the image via deviding all the values by 255\n        image /= 255\n        img_data_array.append(image)\n        #append image to class_name\n        class_name.append(data.loc[index,['type']].values[0])\n    return np.array(img_data_array)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:34:46.499369Z","iopub.execute_input":"2021-08-29T16:34:46.49971Z","iopub.status.idle":"2021-08-29T16:34:46.506861Z","shell.execute_reply.started":"2021-08-29T16:34:46.499677Z","shell.execute_reply":"2021-08-29T16:34:46.506004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Dropout, Flatten,GlobalAveragePooling2D\n#load pretrained ResNet152V2\nResNet152V2 = tf.keras.applications.ResNet152V2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n#freewe the weights \nResNet152V2.trainable = False\n# add custom output layers for ResNet\n#get the output of the model\nx = ResNet152V2.output\nx = GlobalAveragePooling2D()(x)\nx = Flatten()(x)\nx = Dense(units=117, activation='relu')(x)\nx = Dense(units=1080, activation='relu')(x)\nx = Dropout(0.7)(x)\nResNet_output  = Dense(units=2, activation='softmax')(x)\nResNet_model = Model(ResNet152V2.input, ResNet_output)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:35:23.554402Z","iopub.execute_input":"2021-08-29T16:35:23.554728Z","iopub.status.idle":"2021-08-29T16:35:29.529121Z","shell.execute_reply.started":"2021-08-29T16:35:23.554697Z","shell.execute_reply":"2021-08-29T16:35:29.528257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers.merge import concatenate\n#concatenate the outputs of the 2 models\nmerge_layer = concatenate([ResNet_output, BERT_model.output], name='Concatenate')\nout = Dense(1048, activation='relu', name='output_layer')(merge_layer)\nout = Dropout(0.5)(out)\nfinal_model_output = Dense(2, activation='sigmoid',name=\"final_layer\")(out)\nmerged_model = Model([ResNet152V2.input, BERT_model.input], outputs=final_model_output)\nmerged_model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3, beta_1=0.9, beta_2=0.9999999, epsilon=1e-06, decay=0.1),\n                     loss = \"sparse_categorical_crossentropy\" ,\n                     metrics=[\"accuracy\"])\n","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:35:29.5307Z","iopub.execute_input":"2021-08-29T16:35:29.531032Z","iopub.status.idle":"2021-08-29T16:35:29.619622Z","shell.execute_reply.started":"2021-08-29T16:35:29.530996Z","shell.execute_reply":"2021-08-29T16:35:29.618774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel,TFBertForSequenceClassification\nimport numpy as np\nfrom transformers import TFAutoModel\nfrom tensorflow.keras import layers\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, AutoModel\n# Load the BERT tokenizer and the pretrained Arabic base BERT model\nbert_tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n# Load the BERT model and the pretrained Arabic base BERT model\nmodel = TFAutoModel.from_pretrained(\"asafaya/bert-base-arabic\")\n#get the liste of sentences\n\"\"\"\nWhen feeding text data into our model, there are a few things to \nbe aware of. First, we must use tokenizer.encode_plus(...) \nto convert our text into input IDs and attention mask tensors\n\"\"\"\nimport tensorflow as tf\n\ndef tokenize_bert(data):\n    sentences = data['post text'].values\n    seq_len = 512\n    num_samples = len(sentences)\n    #lists of ids\n    input_ids = np.zeros((num_samples, seq_len))\n    #get the lists of masks\n    attention_masks = np.zeros((num_samples, seq_len))\n    for i,sent in enumerate(sentences):\n        #tokenize words in the sentence\n        #return_tensors: return the resultat as tensor\n        #pad_to_max_length :pads the sentences to max length\n        bert_inp=bert_tokenizer.encode_plus(sent,add_special_tokens = True\n                                            ,max_length =seq_len,pad_to_max_length = True,\n                                             truncation=True\n                                            ,return_attention_mask = True,\n                                             return_tensors='tf')  # Return pytorch tensors.\n        #add encoded \n        input_ids[i, :] = bert_inp['input_ids']\n        #applying attention masks\n        attention_masks[i, :] = bert_inp['attention_mask']\n    return input_ids,attention_masks","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:35:37.485645Z","iopub.execute_input":"2021-08-29T16:35:37.488291Z","iopub.status.idle":"2021-08-29T16:35:42.040521Z","shell.execute_reply.started":"2021-08-29T16:35:37.48824Z","shell.execute_reply":"2021-08-29T16:35:42.039527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split = int((80*len(data))/100)\nfrom sklearn.utils import shuffle\n\ndata1 = shuffle(data1)\n#train \ntrain = data1.iloc[:split,:]\ntest = data1.iloc[split:,:]\nimages_train =create_dataset(train)\nimages_test = images=create_dataset(test)\ntrain = train.drop(['post image'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:37:58.480146Z","iopub.execute_input":"2021-08-29T16:37:58.480475Z","iopub.status.idle":"2021-08-29T16:38:00.654562Z","shell.execute_reply.started":"2021-08-29T16:37:58.480444Z","shell.execute_reply":"2021-08-29T16:38:00.653678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:40:03.961294Z","iopub.execute_input":"2021-08-29T16:40:03.961619Z","iopub.status.idle":"2021-08-29T16:40:03.980486Z","shell.execute_reply.started":"2021-08-29T16:40:03.96159Z","shell.execute_reply":"2021-08-29T16:40:03.979264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_inp,train_mask = tokenize_bert(train)\ntest_inp,test_mask = tokenize_bert(test)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:38:44.347334Z","iopub.execute_input":"2021-08-29T16:38:44.347761Z","iopub.status.idle":"2021-08-29T16:38:44.358926Z","shell.execute_reply.started":"2021-08-29T16:38:44.347723Z","shell.execute_reply":"2021-08-29T16:38:44.35811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_inp,train_mask = tokenize_bert(train)\ntest_inp,test_mask= tokenize_bert(test)\nlabels_train = train[['type']]\nlabels_test = test[['type']]\n\nmerged_model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\ntrain_x = [np.array(images_train),[train_inp,train_mask, train.iloc[:,2:]  ]]\nmerged_model.fit(train_x,labels_train, epochs=30, steps_per_epoch=50)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:41:43.282928Z","iopub.execute_input":"2021-08-29T16:41:43.283308Z","iopub.status.idle":"2021-08-29T16:44:20.029497Z","shell.execute_reply.started":"2021-08-29T16:41:43.283277Z","shell.execute_reply":"2021-08-29T16:44:20.028716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_lost ,test_acc_model = merged_model.evaluate(\n[np.array(images_test), [test_inp,test_mask, test.iloc[:,3:].values ]  ],labels_test\n)\nprint(\"The accuracy of the model model is:\",(test_acc_model*100))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:44:31.584246Z","iopub.execute_input":"2021-08-29T16:44:31.584594Z","iopub.status.idle":"2021-08-29T16:44:37.794876Z","shell.execute_reply.started":"2021-08-29T16:44:31.584562Z","shell.execute_reply":"2021-08-29T16:44:37.794065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_metrics(y_pred,y_list):\n    FP,FN,TP,TN=0,0,0,0\n    for i in range(len(y_list)):\n        if y_pred[i]==y_list[i]:\n            if y_pred[i]==1:\n                TP+=1\n            else:\n                TN+=1\n        else:\n            if y_pred[i]==0:\n                FP+=1 \n            else: \n                FN+=1\n    return FP,FN,TP,TN\ndef accuracy(y_pred,y_list):\n    FP,FN,TP,TN=get_metrics(y_pred,y_list)\n    return (TP+TN)/len(y_list)\ndef recall(y_pred,y_list):\n    FP,FN,TP,TN=get_metrics(y_pred,y_list)\n    return TP/(TP+FN)\ndef precision(y_pred,y_list):\n    FP,FN,TP,TN=get_metrics(y_pred,y_list)\n    return TP/(TP+FP)\ndef f1_score(y_pred,y_list):\n    prec,rec=recall(y_pred,y_list),precision(y_pred,y_list)\n    return (2*prec*rec)/(prec+rec)\n\n\ny_list = list( labels_test['type'] )\n\npredicted = merged_model.predict(  [np.array(images_test), [test_inp,test_mask, test.iloc[:,3:].values ]  ] )\ny_pred = list(predicted)\ny_pred = [ list(i).index(max(i[0],i[1])) for i in predicted ]\nprint(accuracy(y_pred,y_list))\nprint(recall(y_pred,y_list))\nprint(precision(y_pred,y_list))\nprint(f1_score(y_pred,y_list))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T16:48:41.088222Z","iopub.execute_input":"2021-08-29T16:48:41.088609Z","iopub.status.idle":"2021-08-29T16:48:41.845264Z","shell.execute_reply.started":"2021-08-29T16:48:41.088573Z","shell.execute_reply":"2021-08-29T16:48:41.84439Z"},"trusted":true},"execution_count":null,"outputs":[]}]}